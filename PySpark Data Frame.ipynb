{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Data Frames Basic\n",
    "\n",
    "We need to import SparkSession then create one to execute spark dataframe commands\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Any Name\").getOrCreate()\n",
    "```\n",
    "Spark dataframes can read many type of files like csv,text,json etc.Its easier to read in csv\n",
    "as it has options like inferSchema-spark will automcatically infer the schema based on the data.header - will take the first row as columns.\n",
    "\n",
    "These options are not there with other formats?\n",
    "\n",
    "```python\n",
    "df = spark.read.csv('test.csv',inferSchema=True,header=True)\n",
    "```\n",
    "\n",
    "Other commands\n",
    "```python\n",
    "df.show()   # shows the table 20 records\n",
    "df.printSchema()   # shows the schema of the data frame\n",
    "df.columns   #shows the list of columns\n",
    "df.describe.show()  #shows statistical summary of numerical columns\n",
    "\n",
    "```\n",
    "\n",
    "To define our own schema\n",
    "\n",
    "``` python\n",
    "from pyspark.sql.types import (StructField,StructField,\n",
    "                             IntegerType,StringType)\n",
    "# giving import in () for multi line\n",
    "\n",
    "data_schema = [StructField('Age',IntegerType(),True),StructField('Name',StringType(),True)]\n",
    "\n",
    "# True means it can have null values\n",
    "\n",
    "final_struc = StructType(fields=data_schema)\n",
    "df = spark.read.csv('test',schema=final_struc)\n",
    "\n",
    "```\n",
    "\n",
    "Selecting columns using select returns dataframe objects.Data frame object has more options\n",
    "so its better to use\n",
    "\n",
    "``` python\n",
    "type(df.select('Channel'))  #this returns a dataframe object\n",
    "type(df['Channel'])  #this returns a column object\n",
    "df.select(['Channel','Region','Milk']).show()  #select multiple columns\n",
    "type(df.head()) - # returns row object\n",
    "df.withColumn('newChannel',df['Channel']*2).show() #new column with some operation on existing column\n",
    "df.withColumn('Channel',df['Channel']*2).show() #some operation on existing column\n",
    "df.withColumnRenamed('newChannel','Channel').show()  #Renaming a existing column\n",
    "\n",
    "```\n",
    "\n",
    "withColumn and withColumnRenamed so make any permanent change.Its just a view. To make permanet change move it to variable.\n",
    "\n",
    "### Sql Queries\n",
    "We can use Sql queries to perform all the operations.This is an option but dataframe options can handle similar operations.We will create a temp table View .Then perform Sql operations on them\n",
    "\n",
    "``` python\n",
    "df.createOrReplaceTempView(\"TempTableName\")\n",
    "results = spark.sql(\"select * from TempTableName\")\n",
    "results.show()\n",
    "\n",
    "```\n",
    "\n",
    "### Basic Sql like operations with dataframe\n",
    "We can do this using the filter.\n",
    "``` python\n",
    "df,\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env1)",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
